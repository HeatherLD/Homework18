{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d26b39",
   "metadata": {},
   "source": [
    "# In-Class Activity, Week 18, Heather Leighton-Dick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97935fb",
   "metadata": {},
   "source": [
    "## 1. Look up the Adam optimization functions in PyTorch https://pytorch.org/docs/stable/optim.html . How does it work? Try at least one other optimization function with the diabetes dataset shown in class. How does the model perform with the new optimizer? Did it perform better or worse than Adam? Why do you think that is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35254017",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b11c15a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df = pd.read_csv(\"../Homework14/diabetes.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d36ea992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = diabetes_df.drop('Outcome', axis=1).values\n",
    "y = diabetes_df['Outcome'].values\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y)\n",
    "\n",
    "# #Standardize\n",
    "sc= StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.fit_transform(X_test)\n",
    "# print(X_train)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1db70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9314,  2.0179,  0.7807,  ...,  0.4315, -0.3748,  0.6321],\n",
      "        [ 0.6326, -1.1486,  0.4654,  ..., -0.1198, -0.2942,  0.7170],\n",
      "        [-0.5625, -0.4769, -0.2703,  ..., -0.2096,  2.7452,  0.0381],\n",
      "        ...,\n",
      "        [-0.8613, -0.7648,  0.0450,  ...,  0.7648, -0.7838, -0.3014],\n",
      "        [ 0.6326,  2.2099,  1.2010,  ...,  0.4315, -0.6047,  2.7537],\n",
      "        [ 0.0351,  0.7385, -0.5856,  ..., -0.3378, -0.5778,  0.2927]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #where the activation functions are\n",
    "\n",
    "#create tensors = matrices \n",
    "X_train = torch.FloatTensor(X_train) \n",
    "X_test = torch.FloatTensor(X_test)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35f8510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aritficial neural network\n",
    "#creating a class using the neural network module\n",
    "class ANN_Model(nn.Module):\n",
    "    \n",
    "    #uses the parameters for nn.Module, check documentation\n",
    "    def __init__(self, input_features=8, hidden1=20, hidden2=20, out_features=2):\n",
    "        \n",
    "        # keyword super is a computed indirect reference, \n",
    "        # iolates changes and makes sure children in the layers of multiple inheritance\n",
    "        # are calling the correct parents\n",
    "        super().__init__() \n",
    "        \n",
    "        self.layer_1_connection = nn.Linear(input_features, hidden1)\n",
    "        self.layer_2_connection = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #apply activation functions\n",
    "        x = F.relu(self.layer_1_connection(x))\n",
    "        x = F.relu(self.layer_2_connection(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "be5b6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "#create instance of model\n",
    "ann = ANN_Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bacb52",
   "metadata": {},
   "source": [
    "### Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "58c5e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Adam(ann.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b810af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = ann.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        #print(f'Epoch number:{epoch} with loss: {loss}')\n",
    "    \n",
    "    # impliment optimizer\n",
    "    # gradient descent - zero the gradient before running backwards propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    #perform optimization step for each epoch\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a5eb22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad(): #decreases memory consumption\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = ann(data)\n",
    "        y_pred.append(prediction.argmax()) #returns index with max element in each prediction set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1b94689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68       150\n",
      "           1       0.50      0.79      0.61        81\n",
      "\n",
      "    accuracy                           0.65       231\n",
      "   macro avg       0.67      0.68      0.65       231\n",
      "weighted avg       0.72      0.65      0.66       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d746b9ba",
   "metadata": {},
   "source": [
    "### Rprop Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a90b64c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = torch.optim.Rprop(ann.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "e08a89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run model through multiple epochs/iterations\n",
    "final_loss = []\n",
    "n_epochs = 500\n",
    "for epoch in range(n_epochs):\n",
    "    y_pred = ann.forward(X_train)\n",
    "    loss = loss_function(y_pred, y_train)\n",
    "    final_loss.append(loss)\n",
    "    \n",
    "    if epoch % 10 == 1:\n",
    "        #print(f'Epoch number:{epoch} with loss: {loss}')\n",
    "    \n",
    "    # impliment optimizer\n",
    "    # gradient descent - zero the gradient before running backwards propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    #perform optimization step for each epoch\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2da9e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad(): #decreases memory consumption\n",
    "    for i, data in enumerate(X_test):\n",
    "        prediction = ann(data)\n",
    "        y_pred.append(prediction.argmax()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d5208b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.64      0.71       150\n",
      "           1       0.51      0.70      0.59        81\n",
      "\n",
      "    accuracy                           0.66       231\n",
      "   macro avg       0.66      0.67      0.65       231\n",
      "weighted avg       0.70      0.66      0.67       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bedcd8",
   "metadata": {},
   "source": [
    "### In terms of recall, the Adam optimizer outperformed Rprop in the Recall (1) class by 9 percentage points (significant in terms of diagnosing people correctly). However, in the Recall (0) class, Rprop performed better. In terms of precision, they performed at about the same level.\n",
    "\n",
    "### Rprop stands for Resilient Propagation and works by adapting the step size (learning rate) for each network weight at each step for each parameter. The Adam (Adaptive Moment Estimation) optimizer takes that part of Rprop and adds it to the way it looks at 1st and 2nd moments (which I think is a way of taking more advantage of the networks/hidden layers (?). Therefore, it makes sense that Rprop and Adam performed fairly similarly, but Adam was a little more able to make definite positive diagnoses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39df19",
   "metadata": {},
   "source": [
    "## 2. Write a function that lists and counts the number of divisors for an input value.\n",
    "Example 1:\n",
    "Input: 5\n",
    "Output: “There are 2 divisors: 1 and 5”\n",
    "Example 2:\n",
    "Input: 40\n",
    "Output: “There are 8 divisors: 1, 2, 4, 5, 8, 10, 20, and 40”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "345bb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factors(x):\n",
    "    factor_list = [x]\n",
    "    \n",
    "    for i in range(1, (x//2 + 1)):\n",
    "        if x % i == 0:\n",
    "            factor_list.append(i)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    print(\"There are \" + str(len(factor_list)) + \" divisors in \" + str(x) + \": \" + str(factor_list[1:i]).lstrip('[').rstrip(']') + \", and \" + str(factor_list[0]) + \".\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a228ac15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 divisors in 44: 1, 2, 4, 11, 22, and 44.\n"
     ]
    }
   ],
   "source": [
    "factors(44)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
